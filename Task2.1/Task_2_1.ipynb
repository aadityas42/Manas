{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "rrcWq-n4EdH-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31a3ac1b"
      },
      "source": [
        "df_test = pd.read_csv('crime_train.csv')\n",
        "if 'weapon' in df_test.columns:\n",
        "    df_test['weapon'] = df_test['weapon'].fillna('Unknown')\n",
        "if 'police_department' in df_test.columns:\n",
        "    df_test['police_department'] = df_test['police_department'].round()\n",
        "    df_test['police_department'] = df_test['police_department'].abs()\n",
        "if 'case_filed' in df_test.columns:\n",
        "    df_test['case_filed'] = pd.to_datetime(df_test['case_filed'], errors='coerce')\n",
        "# Identify numerical columns for standardization\n",
        "numerical_cols = df_test.select_dtypes(include=np.number).columns.tolist()\n",
        "# Exclude 'closed', 'Unnamed: 0', 'Num' if they are in numerical_cols\n",
        "cols_to_exclude = ['closed', 'Unnamed: 0', 'Num']\n",
        "numerical_cols = [col for col in numerical_cols if col not in cols_to_exclude]\n",
        "# Apply Z-score normalization to the numerical columns\n",
        "for col in numerical_cols:\n",
        "    mean = df_test[col].mean()\n",
        "    std = df_test[col].std()\n",
        "    if std > 0:\n",
        "        df_test[col] = (df_test[col] - mean) / std\n",
        "    else:\n",
        "        # If std is 0, all values are the same, so normalization is not needed\n",
        "        df_test[col] = df_test[col] - mean\n",
        "# Encode categorical features\n",
        "categorical_cols = ['city', 'crime_description', 'weapon', 'domain', 'sex']\n",
        "df_test = pd.get_dummies(df_test, columns=categorical_cols, drop_first=True)\n",
        "# Label encode 'closed'\n",
        "if 'closed' in df_test.columns:\n",
        "  df_test['closed'] = df_test['closed'].map({'No': 0, 'Yes': 1})"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c1d0345"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7450ba2"
      },
      "source": [
        "def compute_gradient(X, y, w, b, lambda_=0):\n",
        "  m = X.shape[0]\n",
        "  z = np.dot(X, w) + b\n",
        "  f_wb = sigmoid(z)\n",
        "  error = f_wb - y\n",
        "  dw = (1/m) * np.dot(X.T, error)\n",
        "  dw += (lambda_ / m) * w\n",
        "  db = (1/m) * np.sum(error)\n",
        "  return dw, db"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8aaa4ca"
      },
      "source": [
        "def compute_cost(X, y, w, b, lambda_=0):\n",
        "  m = X.shape[0]\n",
        "  z = np.dot(X, w) + b\n",
        "  f_wb = sigmoid(z)\n",
        "  loss = (-1/m) * np.sum(y * np.log(f_wb) + (1 - y) * np.log(1 - f_wb))\n",
        "  reg_cost = (lambda_ / (2 * m)) * np.sum(w**2)\n",
        "  total_cost = loss + reg_cost\n",
        "  return total_cost"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a8f3693"
      },
      "source": [
        "def train_model(X, y, learning_rate, num_iterations, w=None, b=None, lambda_=0):\n",
        "    m, n = X.shape\n",
        "    if w is None:\n",
        "        w = np.zeros(n)\n",
        "    if b is None:\n",
        "        b = 0\n",
        "    costs = []\n",
        "    for i in range(num_iterations):\n",
        "        dw, db = compute_gradient(X, y, w, b, lambda_)\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "        cost = compute_cost(X, y, w, b, lambda_)\n",
        "        costs.append(cost)\n",
        "        # Print cost and gradients every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "            print(f\"Gradients at iteration {i}: dw={dw[:5]}..., db={db}\") # Print first 5 dw elements\n",
        "    return w, b, costs\n",
        "# Prepare data for training\n",
        "# Convert boolean columns to float before converting to numpy array\n",
        "X_train_np = X_train.astype(float).to_numpy()\n",
        "y_train_np = y_train.to_numpy()\n",
        "learning_rate = 0.000005 # Use the stable learning rate\n",
        "num_iterations = 1000 # number of iterations\n",
        "lambda_value = 0.1  # Small lambda value for regularization\n",
        "# Train the model\n",
        "w_trained, b_trained, training_costs = train_model(X_train_np, y_train_np, learning_rate, num_iterations, lambda_=lambda_value)\n",
        "print(\"Training finished with regularization.\")\n",
        "print(f\"Learned weights: {w_trained}\")\n",
        "print(f\"Learned bias: {b_trained}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "debb93d7"
      },
      "source": [
        "df_test_eval = pd.read_csv('crime_test.csv')\n",
        "if 'weapon' in df_test_eval.columns:\n",
        "    df_test_eval['weapon'] = df_test_eval['weapon'].fillna('Unknown')\n",
        "# Handles 'police_department' column (round off and taking absolute value)\n",
        "if 'police_department' in df_test_eval.columns:\n",
        "    df_test_eval['police_department'] = df_test_eval['police_department'].round()\n",
        "    df_test_eval['police_department'] = df_test_eval['police_department'].abs()\n",
        "# Convert 'case_filed' to datetime\n",
        "if 'case_filed' in df_test_eval.columns:\n",
        "    df_test_eval['case_filed'] = pd.to_datetime(df_test_eval['case_filed'], errors='coerce')\n",
        "# Identify numerical columns for standardization\n",
        "numerical_cols_test = df_test_eval.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Exclude 'closed', 'Unnamed: 0', 'Num' if they are in numerical_cols_test\n",
        "cols_to_exclude = ['closed', 'Unnamed: 0', 'Num']\n",
        "numerical_cols_test = [col for col in numerical_cols_test if col not in cols_to_exclude]\n",
        "\n",
        "for col in numerical_cols_test:\n",
        "    mean_train = df_test[col].mean()\n",
        "    std_train = df_test[col].std()\n",
        "    if std_train > 0:\n",
        "        df_test_eval[col] = (df_test_eval[col] - mean_train) / std_train\n",
        "    else:\n",
        "        df_test_eval[col] = df_test_eval[col] - mean_train\n",
        "\n",
        "categorical_cols = ['city', 'crime_description', 'weapon', 'domain', 'sex']\n",
        "df_test_eval = pd.get_dummies(df_test_eval, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Label encoding the 'closed' column\n",
        "if 'closed' in df_test_eval.columns:\n",
        "  df_test_eval['closed'] = df_test_eval['closed'].map({'No': 0, 'Yes': 1})"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d0b1dbd",
        "outputId": "a6082368-c311-4d05-bf98-921997b6832c"
      },
      "source": [
        "# Identify numerical and boolean columns\n",
        "numerical_cols = df_test.select_dtypes(include=np.number).columns.tolist()\n",
        "boolean_cols = df_test.select_dtypes(include='bool').columns.tolist()\n",
        "\n",
        "# Combining them\n",
        "base_features = numerical_cols + boolean_cols\n",
        "\n",
        "# Exclude the target variable and unrequired factors\n",
        "cols_to_exclude = ['closed', 'Unnamed: 0', 'Num', 'case_filed']\n",
        "base_features = [col for col in base_features if col not in cols_to_exclude]\n",
        "\n",
        "print(\"Base features for polynomial expansion:\")\n",
        "print(base_features)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base features for polynomial expansion:\n",
            "['area', 'age', 'police_department', 'city_Ahmedabad', 'city_Bangalore', 'city_Bhopal', 'city_Chennai', 'city_Delhi', 'city_Faridabad', 'city_Ghaziabad', 'city_Hyderabad', 'city_Indore', 'city_Jaipur', 'city_Kalyan', 'city_Kanpur', 'city_Kolkata', 'city_Lucknow', 'city_Ludhiana', 'city_Meerut', 'city_Mumbai', 'city_Nagpur', 'city_Nashik', 'city_Patna', 'city_Pune', 'city_Rajkot', 'city_Srinagar', 'city_Surat', 'city_Thane', 'city_Varanasi', 'city_Vasai', 'city_Visakhapatnam', 'crime_description_ASSAULT', 'crime_description_BURGLARY', 'crime_description_COUNTERFEITING', 'crime_description_CYBERCRIME', 'crime_description_DOMESTIC VIOLENCE', 'crime_description_DRUG OFFENSE', 'crime_description_EXTORTION', 'crime_description_FIREARM OFFENSE', 'crime_description_FRAUD', 'crime_description_HOMICIDE', 'crime_description_IDENTITY THEFT', 'crime_description_ILLEGAL POSSESSION', 'crime_description_KIDNAPPING', 'crime_description_PUBLIC INTOXICATION', 'crime_description_ROBBERY', 'crime_description_SEXUAL ASSAULT', 'crime_description_SHOPLIFTING', 'crime_description_TRAFFIC VIOLATION', 'crime_description_VANDALISM', 'crime_description_VEHICLE - STOLEN', 'weapon_Explosives', 'weapon_Firearm', 'weapon_Knife', 'weapon_Other', 'weapon_Poison', 'weapon_Unknown', 'domain_Other Crime', 'domain_Traffic Fatality', 'domain_Violent Crime', 'sex_M', 'sex_X']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5c980c9"
      },
      "source": [
        "# Define polynomial degree\n",
        "degree = 2\n",
        "# Creating a copy of the training DataFrame to add the polynomial features\n",
        "df_train_poly = df_test.copy()\n",
        "# Iterate through each base feature and create polynomial features\n",
        "for feature in base_features:\n",
        "    # Ensure the feature is treated as numeric for calculation\n",
        "    df_train_poly[feature] = df_train_poly[feature].astype(float)\n",
        "    for d in range(2, degree + 1):\n",
        "        new_feature_name = f'{feature}_power_{d}'\n",
        "        df_train_poly[new_feature_name] = df_train_poly[feature] ** d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f34a9951"
      },
      "source": [
        "# Create interaction features for unique pairs of base features\n",
        "for i in range(len(base_features)):\n",
        "    for j in range(i + 1, len(base_features)):\n",
        "        feature1 = base_features[i]\n",
        "        feature2 = base_features[j]\n",
        "        new_feature_name = f'{feature1}_x_{feature2}'\n",
        "        df_train_poly[new_feature_name] = df_train_poly[feature1] * df_train_poly[feature2]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0fc9185",
        "outputId": "39beb856-1cf3-4e5e-c59c-3e8aeb982eb1"
      },
      "source": [
        "# Drop the columns 'closed', 'Unnamed: 0', 'Num', and 'case_filed'\n",
        "X_train = df_train_poly.drop(['closed', 'Unnamed: 0', 'Num', 'case_filed'], axis=1)\n",
        "\n",
        "# Convert the X_train DataFrame to a NumPy array with a float data type\n",
        "X_train_np = X_train.astype(float).to_numpy()\n",
        "\n",
        "# Select the 'closed' column to create the target variable vector\n",
        "y_train = df_train_poly['closed']\n",
        "\n",
        "# Convert the y_train Series to a NumPy array\n",
        "y_train_np = y_train.to_numpy()\n",
        "\n",
        "print(\"Shape of X_train_np:\", X_train_np.shape)\n",
        "print(\"Shape of y_train_np:\", y_train_np.shape)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_np: (22489, 2015)\n",
            "Shape of y_train_np: (22489,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d98d0dff"
      },
      "source": [
        "learning_rate = 0.00005\n",
        "num_iterations = 1000 # number of iterations\n",
        "lambda_value = 0.1  # Small lambda value for regularization\n",
        "\n",
        "# Train the model using the expanded feature matrix X_train_np and target vector y_train_np\n",
        "w_trained, b_trained, training_costs = train_model(X_train_np, y_train_np, learning_rate, num_iterations, lambda_=lambda_value)\n",
        "\n",
        "# Print completion message and learned parameters\n",
        "print(\"Training finished with regularization on expanded features.\")\n",
        "# Print only a portion of the weights as they can be numerous\n",
        "print(f\"Learned weights (first 10): {w_trained[:10]}\")\n",
        "print(f\"Learned bias: {b_trained}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a03460b7"
      },
      "source": [
        "# Prepare test data for prediction\n",
        "X_test_poly_np = df_test_eval_poly.to_numpy()\n",
        "# Create a copy of the test DataFrame to add polynomial features\n",
        "df_test_eval_poly = df_test_eval.copy()\n",
        "\n",
        "# Drop non-feature columns from the test data\n",
        "df_test_eval_poly = df_test_eval_poly.drop(['closed', 'Unnamed: 0', 'Num', 'case_filed'], axis=1)\n",
        "\n",
        "# Identify the base features in the test data for polynomial expansion\n",
        "# These should be all columns now in df_test_eval_poly\n",
        "base_features_test = df_test_eval_poly.columns.tolist()\n",
        "\n",
        "# Ensure all base features are treated as numeric for calculation\n",
        "for feature in base_features_test:\n",
        "    df_test_eval_poly[feature] = df_test_eval_poly[feature].astype(float)\n",
        "\n",
        "# Generating polynomial features for test data\n",
        "degree = 2\n",
        "for feature in base_features_test:\n",
        "    for d in range(2, degree + 1):\n",
        "        new_feature_name = f'{feature}_power_{d}'\n",
        "        # Add the new polynomial feature to the test DataFrame\n",
        "        df_test_eval_poly[new_feature_name] = df_test_eval_poly[feature] ** d\n",
        "\n",
        "# Generate interaction terms for test datag\n",
        "for i in range(len(base_features_test)):\n",
        "    for j in range(i + 1, len(base_features_test)):\n",
        "        feature1 = base_features_test[i]\n",
        "        feature2 = base_features_test[j]\n",
        "        new_feature_name = f'{feature1}_x_{feature2}'\n",
        "        # Adding the new interaction feature to the test DataFrame\n",
        "        df_test_eval_poly[new_feature_name] = df_test_eval_poly[feature1] * df_test_eval_poly[feature2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d77db9",
        "outputId": "812e88d1-ba7e-4e23-a8a6-cdb42df7941e"
      },
      "source": [
        "# Make predictions on the expanded test data\n",
        "# Use the weights and bias trained on the expanded features (from cell d98d0dff)\n",
        "test_predictions_poly = predict(X_test_poly_np, w_trained, b_trained)\n",
        "\n",
        "print(\"Predictions on expanded test data:\")\n",
        "print(test_predictions_poly)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions on expanded test data:\n",
            "[1 1 1 ... 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85368b7b",
        "outputId": "a8748f1d-95e9-459f-b671-6bf16f39694a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get the actual values from the test set\n",
        "y_test_actual = df_test_eval['closed'].to_numpy()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_poly = accuracy_score(y_test_actual, test_predictions_poly)\n",
        "\n",
        "print(f\"Model Accuracy on Expanded Test Data: {accuracy_poly:.4f}\")"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Expanded Test Data: 0.5045\n"
          ]
        }
      ]
    }
  ]
}
